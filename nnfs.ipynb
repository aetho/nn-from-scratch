{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "Neural networks comprise of layers of neurons and connections between the neurons of each layer. Tuning the weights and biases of these connections allow the network to \"learn\" and predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron code\n",
    "Suppose we are looking at a single neuron taking in 3 inputs from the previous layer. The following is a simplified look at what a neuron does. It takes a weighted sum of its inputs and adds the bias associated with the neuron. The result of this calculation is output of the neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3] # Output of previous layer's neurons (could be from an actual input layer or a hidden layer)\n",
    "weights = [0.2, 0.8, -0.5] # Strength of connection between the previous layer's neurons\n",
    "bias = 2 # Bias associated with this particular neuron\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Code\n",
    "Now suppose we are are looking at a single layer consisting of 3 neurons taking inputs from a previous layer of 4 neurons. This time there would be 3 sets of weights as well as 3 biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights1 = [0.2, 0.8, -0.5, 1.0]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "output = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,\n",
    "\t\t  inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,\n",
    "\t\t  inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy and Dot Product\n",
    "To make things faster and more concise, we can use the numpy's dot product function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "\t\t   [0.5, -0.91, 0.26, -0.5],\n",
    "\t\t   [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "output = np.dot(weights, inputs) + biases\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching\n",
    "Now consider instead of passing a single input at time, we wish to pass a batch of inputs. Doing this allows us to reduce computation time. The following code details how the output is calculated for a batch of 3 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "\t\t  [2.0, 5.0, -1.0, 2.0],\n",
    "\t\t  [-1.5, 2.7, 3.3, -0.8]]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "\t\t   [0.5, -0.91, 0.26, -0.5],\n",
    "\t\t   [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2, 3, 0.5]\n",
    "output = np.dot(inputs, np.array(weights).T) + biases\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOP\n",
    "Now consider that we wish to add more layers of neurons. The simplest way would be to type out another set of weights and biases but this can be quite restricting when we want to modify the neural network. Thus, we'll be abstracting neurons and layers into classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "\tdef __init__(self, n_inputs, n_neurons):\n",
    "\t\t# Randomly initialise weights to be a small number \n",
    "\t\tself.weight = 0.10*np.random.randn(n_inputs, n_neurons)\n",
    "\t\tself.biases = np.zeros((1, n_neurons))\n",
    "\tdef forward(self, inputs):\n",
    "\t\tself.output = np.dot(inputs, self.weight) + self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: here we set initial biases to be zeros but this can sometimes cause zeros to propagate through the network, resulting in a \"dead\" network. Hence we should consider initial biases when creating/tuning a network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our dense layer abstracted, we can now make and use multiple layers as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04496121  0.0411305  -0.04768364 -0.87879176 -0.3194631 ]\n",
      " [-0.03422296  0.52952711  0.65998613 -0.12569279 -0.54148672]\n",
      " [-0.3462541  -0.63243509  0.39965169 -0.82635177 -0.12040413]]\n",
      "[[ 0.05249861 -0.20362281]\n",
      " [-0.03093784  0.16089939]\n",
      " [-0.01137082 -0.20589618]]\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "X = [[1, 2, 3, 2.5],\n",
    "\t[2.0, 5.0, -1.0, 2.0],\n",
    "\t[-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "layer1 = Layer_Dense(4, 5)\n",
    "layer2 = Layer_Dense(5,2)\n",
    "\n",
    "layer1.forward(X)\n",
    "layer2.forward(layer1.output)\n",
    "\n",
    "print(layer1.output)\n",
    "print(layer2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "Activation functions allow the network to better fit the data. The layer class that we have used so far is considered to be using the idenity activation function, `f(x) = x`. However, to solve more difficult problems we require nonlinear activation functions. Some common/popular nonlinear activation functions include the sigmoid function and the rectified linear unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Unit (ReLU)\n",
    "The ReLU activation function returns `f(x) = x` for `x > 0` and `f(x) = 0` for `x<=0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "\tdef forward(self, inputs):\n",
    "\t\tself.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function can then be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "# 100 feature sets of 3 classes (each feature set containing 2 features (x,y))\n",
    "X,y = spiral_data(100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.         0.00113527 0.00052279 0.         0.        ]\n",
      " [0.         0.00194636 0.         0.00132865 0.        ]\n",
      " ...\n",
      " [0.01747815 0.         0.13669794 0.         0.14357159]\n",
      " [0.01212957 0.02505011 0.16120295 0.         0.13918204]\n",
      " [0.01317252 0.01895553 0.16127414 0.         0.14292897]]\n"
     ]
    }
   ],
   "source": [
    "layer1 = Layer_Dense(2, 5)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "\n",
    "print(activation1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: here we see that there are no negative values and many values have been set to zero as expected of the ReLU function. However, if we find that the network is \"dying\", it could mean that our initial biases may need to be tweaked."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fa36bcf1766e76261d7e2de351ed41c907e2998d874acb8e9d577b33be7ab2b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
